{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "-calculate enters and exits for each 4-hour block\n",
    "\n",
    "Look at:\n",
    "-turnstile volume\n",
    "-can we filter out student swipes?\n",
    "-only look at spring for past couple of years (maybe compare to entire years)\n",
    "-exits in evening: income (where they live); enters in evening: employers (work)\n",
    "\n",
    "For presentation:\n",
    "-top stations (top 10?) and best times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(week_nums):\n",
    "    url = \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt\"\n",
    "    dfs = []\n",
    "    for week_num in week_nums:\n",
    "        file_url = url.format(week_num)\n",
    "        dfs.append(pd.read_csv(file_url))\n",
    "    return pd.concat(dfs)\n",
    "        \n",
    "week_nums = [150307, 150314, 150321, 150328, 160305, 160312, 160319, 160326,  170304, 170311, 170318, 170325]\n",
    "ts = get_data(week_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts.columns = [column.strip() for column in ts.columns]\n",
    "ts = ts[ts.DESC == 'REGULAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts = ts.drop([\"DESC\"], axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts['DATETIME'] = pd.to_datetime(ts['DATE'] + ' ' + ts['TIME'], format=\"%m/%d/%Y %H:%M:%S\")+ pd.Timedelta(minutes = 30)\n",
    "ts['DATETIME'] = ts['DATETIME'].apply(lambda L: pd.datetime(L.year, L.month, L.day, L.hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ts.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATETIME\"], inplace=True)\n",
    "#Should not be a big deal. Number of times we round up is low and difference in entries / exits is also probably low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Need to check that next row is 4 hours later. shift \"Next DATETIME\". Check occurences of it. Doesn't matter if < 4.\n",
    "#The worry is that there's a large difference in times, such as 5 AM -> 5 PM which would give a very large delta.\n",
    "ts[[\"NEXT_EXITS\", \"NEXT_ENTRIES\"]] = (ts.groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\"])[\"EXITS\", \"ENTRIES\"]\n",
    "                                                       .transform(lambda grp: grp.shift(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts.dropna(subset=[\"NEXT_EXITS\"], axis=0, inplace=True)\n",
    "ts['DELTA_ENTRIES'] = ts['NEXT_ENTRIES']-ts['ENTRIES']\n",
    "ts['DELTA_EXITS'] = ts['NEXT_EXITS']-ts['EXITS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_delta_counts(row, max_counter,higher,lower):\n",
    "    counter = row[higher] - row[lower]\n",
    "    if counter < 0:\n",
    "        # May be counter is reversed?\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "        #print(row[higher], row[lower])\n",
    "        #Looks like a reset\n",
    "        counter = min(row[higher], row[lower])\n",
    "    if counter > max_counter:\n",
    "        # Check it again to make sure we are not giving a counter that's too big\n",
    "        return 0\n",
    "    return counter\n",
    "\n",
    "ts[\"DELTA_ENTRIES\"] = ts.apply(get_delta_counts, axis=1, max_counter=10000,higher = 'NEXT_ENTRIES',lower = 'ENTRIES')\n",
    "ts[\"DELTA_EXITS\"] = ts.apply(get_delta_counts, axis=1, max_counter=10000,higher = 'NEXT_EXITS',lower = 'EXITS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts['DOTW']= ts['DATETIME'].dt.dayofweek\n",
    "ts['IS_WEEKDAY']=True\n",
    "ts.loc[(ts['DOTW'] == 5) | (ts['DOTW']==6),'IS_WEEKDAY'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts['TIMET'] = pd.to_datetime(ts['TIME'])+ pd.Timedelta(minutes = 30)\n",
    "ts['TIMET'] = ts['TIMET'].apply(lambda L: pd.datetime(L.year, L.month, L.day, L.hour))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts['HOUR']=pd.DatetimeIndex(ts['DATETIME']).hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ts.loc[(ts['HOUR'] < 12) & (ts['HOUR']>= 4),'TOD'] = 'MORNING'\n",
    "ts.loc[(12<=ts['HOUR']) & (ts['HOUR'] < 20),'TOD'] = 'EVENING'\n",
    "ts.dropna(subset=[\"TOD\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entries = ts.groupby(['STATION','LINENAME','TOD','IS_WEEKDAY','DATE'])['DELTA_ENTRIES'].sum().reset_index().groupby(['STATION','LINENAME','TOD','IS_WEEKDAY']).mean()\n",
    "ent_df = entries.reset_index()\n",
    "exits = ts.groupby(['STATION','LINENAME','TOD','IS_WEEKDAY','DATE'])['DELTA_EXITS'].sum().reset_index().groupby(['STATION','LINENAME','TOD','IS_WEEKDAY']).mean()\n",
    "exi_df = exits.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ent_top_eve_wd=ent_df[(ent_df['TOD']=='EVENING') & (ent_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_ENTRIES', ascending=False).head(40)\n",
    "ent_top_mor_wd=ent_df[(ent_df['TOD']=='MORNING') & (ent_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_ENTRIES', ascending=False).head(40)\n",
    "ent_top_mor_we=ent_df[(ent_df['TOD']=='MORNING') & (ent_df['IS_WEEKDAY'] == 0)].sort_values(by = 'DELTA_ENTRIES', ascending=False).head(40)\n",
    "ent_top_eve_we=ent_df[(ent_df['TOD']=='EVENING') & (ent_df['IS_WEEKDAY'] == 0)].sort_values(by = 'DELTA_ENTRIES', ascending=False).head(40)\n",
    "exi_top_eve_wd=exi_df[(exi_df['TOD']=='EVENING') & (exi_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_EXITS', ascending=False).head(40)\n",
    "exi_top_mor_wd=exi_df[(exi_df['TOD']=='MORNING') & (exi_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_EXITS', ascending=False).head(40)\n",
    "exi_top_eve_we=exi_df[(exi_df['TOD']=='EVENING') & (exi_df['IS_WEEKDAY']==0)].sort_values(by = 'DELTA_EXITS', ascending=False).head(40)\n",
    "exi_top_mor_we=exi_df[(exi_df['TOD']=='MORNING') & (exi_df['IS_WEEKDAY']==0)].sort_values(by = 'DELTA_EXITS', ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ent_top_eve_wd.to_csv('entries_evening_weekday.csv')\n",
    "ent_top_mor_wd.to_csv('entries_morning_weekday.csv')\n",
    "ent_top_eve_we.to_csv('entries_evening_weekend.csv')\n",
    "ent_top_mor_we.to_csv('entries_morning_weekend.csv')\n",
    "exi_top_eve_wd.to_csv('exits_evening_weekday.csv')\n",
    "exi_top_mor_wd.to_csv('exits_morning_weekday.csv')\n",
    "exi_top_eve_we.to_csv('exits_evening_weekend.csv')\n",
    "exi_top_mor_we.to_csv('exits_morning_weekend.csv')\n",
    "\n",
    "#bus_day=pd.merge(ent_top_eve_wd,exi_top_mor_wd,on=['STATION','LINENAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ent_eve_wd=ent_df[(ent_df['TOD']=='EVENING') & (ent_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_ENTRIES', ascending=False)\n",
    "exi_mor_wd=exi_df[(exi_df['TOD']=='MORNING') & (exi_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_EXITS', ascending=False)\n",
    "ent_mor_wd=ent_df[(ent_df['TOD']=='MORNING') & (ent_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_ENTRIES', ascending=False)\n",
    "exi_eve_wd=exi_df[(exi_df['TOD']=='EVENING') & (exi_df['IS_WEEKDAY'])].sort_values(by = 'DELTA_EXITS', ascending=False)\n",
    "liv_day=pd.merge(ent_mor_wd,exi_eve_wd,on=['STATION','LINENAME'])\n",
    "liv_day['AVERAGE_DAILY']=bus_day['DELTA_ENTRIES']+bus_day['DELTA_EXITS']\n",
    "liv_day_top = liv_day.sort_values(by = 'AVERAGE_DAILY', ascending=False).head(60)\n",
    "liv_day_top.to_csv('liv_day_top.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus_day=pd.merge(ent_eve_wd,exi_mor_wd,on=['STATION','LINENAME'])\n",
    "bus_day['AVERAGE_DAILY']=bus_day['DELTA_ENTRIES']+bus_day['DELTA_EXITS']\n",
    "bus_day_top = bus_day.sort_values(by = 'AVERAGE_DAILY', ascending=False).head(40)\n",
    "bus_day_top.to_csv('bus_day_top.csv')\n",
    "bus_day.to_csv('bus_day_daily.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entries_t = test_df.groupby(['STATION','LINENAME','TOD','IS_WEEKDAY','DATE'])['DELTA_ENTRIES'].sum().reset_index().groupby(['STATION','LINENAME','TOD','IS_WEEKDAY']).mean()\n",
    "ent_df_t = entries_t.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Specifically look at 14th street station.\n",
    "bus_day[bus_day['STATION'] == '14 ST'].sort_values(by = 'AVERAGE_DAILY', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
